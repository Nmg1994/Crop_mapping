{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nmg1994/Crop_mapping/blob/main/Crop_mapping_Region_based_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Intalling necessary packages"
      ],
      "metadata": {
        "id": "KgBf6TkBPIp5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGq9m0exOMJ0"
      },
      "outputs": [],
      "source": [
        "!pip install rasterio\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing necessary libraries/modules"
      ],
      "metadata": {
        "id": "5oyOTX6APRiQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JfpJjlPhTAPO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pylab as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import sklearn.metrics\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Flatten, Concatenate, Layer, Conv1D, MaxPooling1D\n",
        "from keras.callbacks import EarlyStopping\n",
        "import pickle\n",
        "from scipy.stats import ks_2samp\n",
        "from itertools import combinations"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mounting Google Drive"
      ],
      "metadata": {
        "id": "pVQKfvyDPlHx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i52NshMsTNdY",
        "outputId": "c42719ef-e50d-4bc2-946e-a475bc94ef84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading training, validation, and test datasets as well as the class weight"
      ],
      "metadata": {
        "id": "eR55ifFePuzU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2IBHtc1f3Ee7"
      },
      "outputs": [],
      "source": [
        "# loading samples along with their corresponding labels # this is done to save some RAM due to the huge size of data\n",
        "train_indices = np.load('/content/drive/My Drive/DL_samples/train_indices.npy')\n",
        "val_indices = np.load('/content/drive/My Drive/DL_samples/val_indices.npy')\n",
        "test_indices = np.load('/content/drive/My Drive/DL_samples/test_indices.npy')\n",
        "\n",
        "train_labels = np.load('/content/drive/My Drive/DL_samples/train_labels.npy')\n",
        "val_labels = np.load('/content/drive/My Drive/DL_samples/val_labels.npy')\n",
        "test_labels = np.load('/content/drive/My Drive/DL_samples/test_labels.npy')\n",
        "\n",
        "\n",
        "train_samples = np.load('/content/drive/My Drive/DL_samples/train_samples.npy')\n",
        "val_samples = np.load('/content/drive/My Drive/DL_samples/val_samples.npy')\n",
        "test_samples = np.load('/content/drive/My Drive/DL_samples/test_samples.npy')\n",
        "\n",
        "# Loading the class weight dictionary from the Google Drive\n",
        "with open('/content/drive/My Drive/DL_samples/class_weights_dict.pkl', 'rb') as f:\n",
        "  class_weights_dict = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assessing whether the datasets conform the IID assumption"
      ],
      "metadata": {
        "id": "5K1z0L-4XNuF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2m94EUZoTTZE"
      },
      "outputs": [],
      "source": [
        "# Investigating the conformity of the datasets to the IID assumption\n",
        "# Calculating class distributions for each dataset\n",
        "train_class_distribution = np.bincount(train_labels) / len(train_labels)\n",
        "val_class_distribution = np.bincount(val_labels) / len(val_labels)\n",
        "test_class_distribution = np.bincount(test_labels) / len(test_labels)\n",
        "\n",
        "# Printing class distributions for each dataset\n",
        "print(\"Training set class distribution:\", train_class_distribution)\n",
        "print(\"Validation set class distribution:\", val_class_distribution)\n",
        "print(\"Testing set class distribution:\", test_class_distribution)\n",
        "\n",
        "# Defining a function to perform Kolmogorov-Smirnov test and Chi-squared test\n",
        "def perform_tests(labels1, labels2, dataset_name1, dataset_name2):\n",
        "    # Performing Kolmogorov-Smirnov test\n",
        "    ks_statistic, p_value_ks = ks_2samp(labels1, labels2)\n",
        "    print(f\"Kolmogorov-Smirnov test statistic for {dataset_name1} vs. {dataset_name2} datasets:\", ks_statistic)\n",
        "    print(f\"p-value for Kolmogorov-Smirnov test between {dataset_name1} vs. {dataset_name2} datasets:\", p_value_ks)\n",
        "\n",
        "# Performing pairwise comparisons between all three datasets\n",
        "datasets = [(\"Training\", train_labels), (\"Validation\", val_labels), (\"Test\", test_labels)]\n",
        "for dataset1, dataset2 in combinations(datasets, 2):\n",
        "    dataset_name1, labels1 = dataset1\n",
        "    dataset_name2, labels2 = dataset2\n",
        "    perform_tests(labels1, labels2, dataset_name1, dataset_name2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing the datasets and their corresponding labels"
      ],
      "metadata": {
        "id": "OrDQJ7YsUAoZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyt8gTbaFO4M"
      },
      "outputs": [],
      "source": [
        "# One-hot encoding labels\n",
        "labels = np.concatenate([train_labels, val_labels, test_labels])\n",
        "print('labels shape:', labels.shape)\n",
        "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "one_hot_encoded = encoder.fit_transform(labels.reshape(-1, 1))\n",
        "one_hot_array = np.array(one_hot_encoded)\n",
        "print('one_hot_array shape:', one_hot_array.shape)\n",
        "\n",
        "train_labels_encoded = one_hot_array[:len(train_labels),:]\n",
        "val_labels_encoded = one_hot_array[len(train_labels): (len(train_labels) + len(val_labels)),:]\n",
        "test_labels_encoded = one_hot_array[(len(train_labels) + len(val_labels)):,:]\n",
        "\n",
        "print('train_labels_encoded shape:', train_labels_encoded.shape)\n",
        "print('val_labels_encoded shape:', val_labels_encoded.shape)\n",
        "print('test_labels_encoded shape:', test_labels_encoded.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nBPxpzPDgI9i"
      },
      "outputs": [],
      "source": [
        "# Defining function for the calculation of indices\n",
        "def indices_calculation(DL_samples_array):\n",
        "\n",
        "  # Extracting individual bands\n",
        "  S2_Blue = DL_samples_array[:,0,:]  # Blue band\n",
        "  S2_Green = DL_samples_array[:,1,:]  # Green band\n",
        "  S2_Red = DL_samples_array[:,2,:]  # Red band\n",
        "  S2_NIR = DL_samples_array[:,3,:]  # NIR band\n",
        "\n",
        "  # Calculation of vegetation indices (VI), NDWI, and BI\n",
        "  # NDVI\n",
        "  NDVI = ((S2_NIR - S2_Red) / (S2_NIR + S2_Red))\n",
        "  NDVI_reshaped = np.expand_dims(NDVI, axis=1)  # Adding a new axis to make it 3D\n",
        "  DL_samples_array = np.concatenate([DL_samples_array,NDVI_reshaped], axis = 1)\n",
        "\n",
        "  # GNDVI\n",
        "  GNDVI = ((S2_NIR - S2_Green) / (S2_NIR + S2_Green))\n",
        "  GNDVI_reshaped = np.expand_dims(GNDVI, axis=1)  # Adding a new axis to make it 3D\n",
        "  DL_samples_array = np.concatenate([DL_samples_array,GNDVI_reshaped], axis = 1)\n",
        "\n",
        "  # GSAVI\n",
        "  GSAVI = ((S2_NIR - S2_Green) / (S2_NIR + S2_Green + 0.5)) * 1.5\n",
        "  GSAVI_reshaped = np.expand_dims(GSAVI, axis=1)  # Adding a new axis to make it 3D\n",
        "  DL_samples_array = np.concatenate([DL_samples_array,GSAVI_reshaped], axis = 1)\n",
        "\n",
        "  # GOSAVI\n",
        "  GOSAVI = ((S2_NIR - S2_Green) / (S2_NIR + S2_Green+ 0.16))\n",
        "  GOSAVI_reshaped = np.expand_dims(GOSAVI, axis=1)  # Adding a new axis to make it 3D\n",
        "  DL_samples_array = np.concatenate([DL_samples_array,GOSAVI_reshaped], axis = 1)\n",
        "\n",
        "  # SAVI\n",
        "  SAVI = ((S2_NIR - S2_Red) / (S2_NIR + S2_Red + 0.5)) * 1.5\n",
        "  SAVI_reshaped = np.expand_dims(SAVI, axis=1)  # Adding a new axis to make it 3D\n",
        "  DL_samples_array = np.concatenate([DL_samples_array,SAVI_reshaped], axis = 1)\n",
        "\n",
        "  # OSAVI\n",
        "  OSAVI = ((S2_NIR - S2_Red) / (S2_NIR + S2_Red + 0.16)) * 1.16\n",
        "  OSAVI_reshaped = np.expand_dims(OSAVI, axis=1)  # Adding a new axis to make it 3D\n",
        "  DL_samples_array = np.concatenate([DL_samples_array,OSAVI_reshaped], axis = 1)\n",
        "\n",
        "  # NDWI\n",
        "  NDWI = ((S2_Green - S2_NIR) / (S2_Green + S2_NIR))\n",
        "  NDWI_reshaped = np.expand_dims(NDWI, axis=1)  # Adding a new axis to make it 3D\n",
        "  DL_samples_array = np.concatenate([DL_samples_array,NDWI_reshaped], axis = 1)\n",
        "\n",
        "  # BI\n",
        "  BI = np.sqrt((S2_Blue ** 2) + (S2_Green ** 2) + (S2_NIR ** 2) + (S2_Red ** 2))\n",
        "  BI_reshaped = np.expand_dims(BI, axis=1)  # Adding a new axis to make it 3D\n",
        "  DL_samples_array = np.concatenate([DL_samples_array,BI_reshaped], axis = 1)\n",
        "\n",
        "  # EVI\n",
        "  EVI = np.where((S2_NIR + 6 * S2_Red - 7.5 * S2_Blue + 1) != 0, 2.5 * ((S2_NIR - S2_Red) / (S2_NIR + 6 * S2_Red - 7.5 * S2_Blue + 1)), 0)\n",
        "  EVI_reshaped = np.expand_dims(EVI, axis=1)  # Adding a new axis to make it 3D\n",
        "  DL_samples_array = np.concatenate([DL_samples_array,EVI_reshaped], axis = 1)\n",
        "\n",
        "  return DL_samples_array\n",
        "\n",
        "# Standardizing each band along with the indices calculated\n",
        "def standarized_function(DL_samples):\n",
        "  for j in range(DL_samples.shape[1]):\n",
        "    scaler = StandardScaler()\n",
        "    each_band_standarized = scaler.fit_transform(DL_samples[:,j,:])\n",
        "    each_band_standarized_reshaped = np.expand_dims(each_band_standarized, axis=1)\n",
        "\n",
        "    if j == 0:\n",
        "      Standarizad_samples = each_band_standarized_reshaped\n",
        "    else:\n",
        "      Standarizad_samples= np.concatenate([Standarizad_samples, each_band_standarized_reshaped], axis = 1)\n",
        "  print('Standarizad_samples shape is: ', Standarizad_samples.shape)\n",
        "  return Standarizad_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZZ5PxZtcsdd"
      },
      "outputs": [],
      "source": [
        "# Applying indices_calculation function to calculate the desired indices\n",
        "# This cell only must be run in the cases whose objective is to include VIs, NDWI, and BI in the process\n",
        "train_samples_all_indices = indices_calculation(train_samples)\n",
        "val_samples_all_indices = indices_calculation(val_samples)\n",
        "test_samples_all_indices = indices_calculation(test_samples)\n",
        "\n",
        "# Standardizing the indices calculated\n",
        "Standarizad_train_samples = standarized_function(train_samples_all_indices)\n",
        "Standarizad_val_samples = standarized_function(val_samples_all_indices)\n",
        "Standarizad_test_samples = standarized_function(test_samples_all_indices)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Considering EVI as the only spectral feature\n",
        "# This cell only must be run in the cases whose objective is to consider the EVI as the only spectral feature in the models\n",
        "train_samples_all_indices = indices_calculation(train_samples)\n",
        "val_samples_all_indices = indices_calculation(val_samples)\n",
        "test_samples_all_indices = indices_calculation(test_samples)\n",
        "\n",
        "# Standardizing the indices calculated\n",
        "Standarizad_train_samples = standarized_function(train_samples_all_indices)\n",
        "Standarizad_val_samples = standarized_function(val_samples_all_indices)\n",
        "Standarizad_test_samples = standarized_function(test_samples_all_indices)\n",
        "\n",
        "# Considering EVI as the only spectral feature\n",
        "Standarizad_train_samples = Standarizad_train_samples[:,-1,:]\n",
        "Standarizad_val_samples = Standarizad_val_samples[:,-1,:]\n",
        "Standarizad_test_samples = Standarizad_test_samples[:,-1,:]\n",
        "\n",
        "# Reshaping the arrays in a suitable shape for being fed in the models\n",
        "Standarizad_train_samples = Standarizad_train_samples.reshape(Standarizad_train_samples.shape[0], 1, Standarizad_train_samples.shape[1])\n",
        "Standarizad_val_samples = Standarizad_val_samples.reshape(Standarizad_val_samples.shape[0], 1, Standarizad_val_samples.shape[1])\n",
        "Standarizad_test_samples = Standarizad_test_samples.reshape(Standarizad_test_samples.shape[0], 1, Standarizad_test_samples.shape[1])"
      ],
      "metadata": {
        "id": "ns8w9si61IRo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transposing the spectral and temporal domains to apply the convolutions on the spectral domain\n",
        "# This cell must be run only in the cases whose objective is to apply convolutions over the spectral domain (i.e., using 1DSpecCNNs model)\n",
        "Standarizad_train_samples= np.transpose(Standarizad_train_samples, (0,2,1))\n",
        "Standarizad_val_samples= np.transpose(Standarizad_val_samples, (0,2,1))\n",
        "Standarizad_test_samples= np.transpose(Standarizad_test_samples, (0,2,1))"
      ],
      "metadata": {
        "id": "iN95kDnoAI55"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modifying the initial architecures of each DL model and exploring the one with optimal performance and using them for the prediction and accuracy assessment on the testing dataset"
      ],
      "metadata": {
        "id": "QqG1BH-o54SO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defininf the LSTM model with the optimal architecture"
      ],
      "metadata": {
        "id": "73WD52JMZ6_g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozAW46D2xn48"
      },
      "outputs": [],
      "source": [
        "# LSTM model\n",
        "def LSTM_classification(Features_training_LSTM, Features_val_LSTM, Features_testing_LSTM, landcover_training_LSTM, landcover_val_LSTM, landcover_testing_LSTM, time_steps, num_features, num_classes, Prediction_only = False):\n",
        "\n",
        "  # Defining early stopping callback\n",
        "  early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "  # Designing the architecture for the LSTM model\n",
        "  LSTM_model = Sequential()\n",
        "  LSTM_model.add(LSTM(units = 256, return_sequences= True, input_shape=(num_features, time_steps)))\n",
        "  LSTM_model.add(Dropout(0.1))\n",
        "  LSTM_model.add(LSTM(units = 128, return_sequences= True))\n",
        "  LSTM_model.add(Dropout(0.1))\n",
        "  LSTM_model.add(Flatten())\n",
        "  LSTM_model.add(Dense(units = 512, activation = 'relu'))\n",
        "  LSTM_model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "  learning_rate = 0.001\n",
        "  adam_optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "  # Compiling the LSTM model\n",
        "  LSTM_model.compile(optimizer= adam_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  # Training the LSTM model\n",
        "  LSTM_history = LSTM_model.fit(Features_training_LSTM, landcover_training_LSTM, epochs=100, batch_size=32, validation_data=(Features_val_LSTM, landcover_val_LSTM),  callbacks=[early_stopping])\n",
        "\n",
        "  # Access the stopped epoch\n",
        "  final_epoch = early_stopping.stopped_epoch\n",
        "  print(\"Training stopped at epoch:\", final_epoch + 1)\n",
        "\n",
        "  sns.set(font_scale=1)\n",
        "  sns.set_style(\"darkgrid\")\n",
        "  plt.figure(figsize=(15, 4))\n",
        "\n",
        "  print('The average accuracy over all epochs: ', sum(LSTM_history.history['val_accuracy']) / len(LSTM_history.history['val_accuracy']))\n",
        "\n",
        "  # Plotting training accuracy\n",
        "  sns.lineplot(x=range(1, final_epoch + 2), y= LSTM_history.history['accuracy'], label='Training accuracy')\n",
        "\n",
        "  # Plotting validation accuracy\n",
        "  sns.lineplot(x=range(1, final_epoch + 2), y= LSTM_history.history['val_accuracy'], label='Validation accuracy')\n",
        "\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.title('Training and Validation accuracy of LSTM model Over Epochs')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  LSTM_model.summary()\n",
        "\n",
        "\n",
        "  #Testing Accuracy\n",
        "  landcover_testing_predicted = LSTM_model.predict(Features_testing_LSTM)\n",
        "  # Finding the index of the highest probability for each sample\n",
        "  max_prob_indices = np.argmax(landcover_testing_predicted, axis=1)\n",
        "\n",
        "  # Creating a binary array with the same shape as landcover_testing_predicted\n",
        "  binary_array = np.zeros_like(landcover_testing_predicted)\n",
        "\n",
        "  # Setting the value of 1 at the index of the highest probability for each sample\n",
        "  binary_array[np.arange(len(landcover_testing_predicted)), max_prob_indices] = 1\n",
        "\n",
        "\n",
        "  if Prediction_only == False:\n",
        "    print(\"Recall score for test dataset: \", sklearn.metrics.recall_score(landcover_testing_LSTM, binary_array, average = 'macro'))\n",
        "    print(\"Precision score for test dataset: \", sklearn.metrics.precision_score(landcover_testing_LSTM, binary_array, average = 'macro'))\n",
        "    print(\"F1 score for test dataset: \", sklearn.metrics.f1_score(landcover_testing_LSTM, binary_array, average = 'macro'))\n",
        "    print(\"ROC-AUC score for test dataset: \", sklearn.metrics.roc_auc_score(landcover_testing_LSTM, binary_array, average = 'macro', multi_class = 'ovr'))\n",
        "\n",
        "    print(sklearn.metrics.classification_report(landcover_testing_LSTM, binary_array))\n",
        "\n",
        "    # Creating an empty dictionary to store ROC AUC scores for each class\n",
        "    roc_auc_scores = {}\n",
        "\n",
        "    # Iterating over each class\n",
        "    for class_index in range(num_classes):\n",
        "        # Extracting the binary predictions for the current class\n",
        "        binary_predictions_class = binary_array[:, class_index]\n",
        "        true_labels_class = landcover_testing_LSTM[:, class_index]\n",
        "\n",
        "        # Computing the ROC-AUC score for the current class\n",
        "        roc_auc = sklearn.metrics.roc_auc_score(true_labels_class, binary_predictions_class)\n",
        "\n",
        "        # Storing the ROC AUC score for the current class in the dictionary\n",
        "        roc_auc_scores[f'Class {class_index}'] = roc_auc\n",
        "\n",
        "    # Printing ROC-AUC scores for each class\n",
        "    for class_name, roc_auc in roc_auc_scores.items():\n",
        "        print(f'ROC AUC for {class_name}: {roc_auc}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying LSTM model"
      ],
      "metadata": {
        "id": "FfmlIyxcZa1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Following the goal set the inputs of the LSTM model:\n",
        "# If only four bands are being used, use the inputs train_samples, val_samples, test_samples; other wise use the inputs Standarizad_train_samples, Standarizad_val_samples, Standarizad_test_samples\n",
        "# Also, do not forget to correspondingly change the time_steps and num_features parameters\n",
        "LSTM_classification(Features_training_LSTM = train_samples, Features_val_LSTM = val_samples, Features_testing_LSTM = test_samples, landcover_training_LSTM = train_labels_encoded, landcover_val_LSTM = val_labels_encoded, landcover_testing_LSTM = test_labels_encoded, time_steps = train_samples.shape[2], num_features = train_samples.shape[1], num_classes = 13, Prediction_only = False)\n"
      ],
      "metadata": {
        "id": "LLCdKw4UYEqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defininf the inception layer for the CNNs models"
      ],
      "metadata": {
        "id": "_vD2TjGgZkFc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0iaNTxlRpEJq"
      },
      "outputs": [],
      "source": [
        "# Designing an inception layer for the CNNs models\n",
        "class InceptionLayer(Layer):\n",
        "    def __init__(self, filters, **kwargs):\n",
        "        super(InceptionLayer, self).__init__(**kwargs)\n",
        "        self.filters = filters\n",
        "        # Define layers here to avoid tf.function issues\n",
        "        self.conv1_3 = Conv1D(filters[0], 3, padding='same', activation='relu')\n",
        "        self.conv1_1 = Conv1D(filters[1], 1, padding='same', activation='relu')\n",
        "\n",
        "        self.conv2_1 = Conv1D(filters[2], 1, padding='same', activation='relu')\n",
        "        self.conv2_3 = Conv1D(filters[3], 3, padding='same', activation='relu')\n",
        "\n",
        "        self.conv3_3 = Conv1D(filters[4], 3, padding='same', activation='relu')\n",
        "        self.conv3_5 = Conv1D(filters[5], 5, padding='same', activation='relu')\n",
        "\n",
        "        self.conv4_5 = Conv1D(filters[6], 5, padding='same', activation='relu')\n",
        "        self.conv4_3 = Conv1D(filters[7], 3, padding='same', activation='relu')\n",
        "\n",
        "        self.conv5_5 = Conv1D(filters[8], 5, padding='same', activation='relu')\n",
        "        self.conv5_1 = Conv1D(filters[9], 1, padding='same', activation='relu')\n",
        "\n",
        "        self.conv6_1 = Conv1D(filters[10], 1, padding='same', activation='relu')\n",
        "        self.conv6_5 = Conv1D(filters[11], 5, padding='same', activation='relu')\n",
        "\n",
        "        self.pool = MaxPooling1D(pool_size=2, strides=1, padding='same')\n",
        "        self.conv_pool = Conv1D(filters[12], 3, padding='same', activation='relu')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Branch 1: 3 Convolution followed by 1 Convolution\n",
        "        branch1 = self.conv1_3(inputs)\n",
        "        branch1 = self.conv1_1(branch1)\n",
        "\n",
        "        # Branch 2: 1 Convolution followed by 3 Convolution\n",
        "        branch2 = self.conv2_1(inputs)\n",
        "        branch2 = self.conv2_3(branch2)\n",
        "\n",
        "        # Branch 3: 3 Convolution followed by 5 Convolution\n",
        "        branch3 = self.conv3_3(inputs)\n",
        "        branch3 = self.conv3_5(branch3)\n",
        "\n",
        "        # Branch 4: 5 Convolution followed by 3 Convolution\n",
        "        branch4 = self.conv4_5(inputs)\n",
        "        branch4 = self.conv4_3(branch4)\n",
        "\n",
        "        # Branch 5: 5 Convolution followed by 1 Convolution\n",
        "        branch5 = self.conv5_5(inputs)\n",
        "        branch5 = self.conv5_1(branch5)\n",
        "\n",
        "        # Branch 6: 1 Convolution followed by 5 Convolution\n",
        "        branch6 = self.conv6_1(inputs)\n",
        "        branch6 = self.conv6_5(branch6)\n",
        "\n",
        "        # Branch pool: Max pooling followed by 3 Convolution\n",
        "        branch_pool = self.pool(inputs)\n",
        "        branch_pool = self.conv_pool(branch_pool)\n",
        "\n",
        "        # Concatenate the outputs of the branches\n",
        "        output = Concatenate(axis=-1)([branch1, branch4, branch5])\n",
        "\n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0], input_shape[1], sum(self.filters)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the 1DTempCNNs model with the optimal architecture"
      ],
      "metadata": {
        "id": "ZFPtM31hZdxj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGNn-oj3RHO3"
      },
      "outputs": [],
      "source": [
        "# 1DTempCNNs model\n",
        "def TempCNNs(Features_training_CNN, Features_val_CNN, Features_testing_CNN, landcover_training_CNN, landcover_val_CNN, landcover_testing_CNN, time_steps, num_features, num_classes, Prediction_only = False):\n",
        "\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "    # Designing the architecture for the 1DTempCNNs model\n",
        "    CNN_model = Sequential()\n",
        "\n",
        "    CNN_model.add(Conv1D(128, kernel_size=3, activation='relu', padding='same', input_shape=(num_features, time_steps)))\n",
        "    CNN_model.add(Dropout(0.1))\n",
        "    CNN_model.add(InceptionLayer((64, 64, 128, 128, 256, 256, 128, 128, 64, 64, 256, 128, 128)))\n",
        "    CNN_model.add(Dropout(0.1))\n",
        "    CNN_model.add(Conv1D(128, kernel_size=3, activation='relu', padding='same'))\n",
        "    CNN_model.add(Dropout(0.1))\n",
        "    CNN_model.add(Conv1D(128, kernel_size=3, activation='relu', padding='same'))\n",
        "    CNN_model.add(Dropout(0.1))\n",
        "    # Flatten layer\n",
        "    CNN_model.add(Flatten())\n",
        "\n",
        "    # Fully connected layers\n",
        "    CNN_model.add(Dense(512, activation='relu'))\n",
        "    # Output layer\n",
        "    CNN_model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    # Compiling the 1DTempCNNs model\n",
        "    CNN_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Training the 1DTempCNNs model\n",
        "    CNN_history = CNN_model.fit(Features_training_CNN, landcover_training_CNN, epochs=100, batch_size=32, validation_data=(Features_val_CNN, landcover_val_CNN),  callbacks=[early_stopping])\n",
        "\n",
        "    # Accessing the stopped epoch\n",
        "    final_epoch = early_stopping.stopped_epoch\n",
        "    print(\"Training stopped at epoch:\", final_epoch + 1)\n",
        "\n",
        "    sns.set(font_scale=1)\n",
        "    sns.set_style(\"darkgrid\")\n",
        "    plt.figure(figsize=(15, 4))\n",
        "\n",
        "    print('The average accuracy over all epochs: ', sum(CNN_history.history['val_accuracy']) / len(CNN_history.history['val_accuracy']))\n",
        "\n",
        "    # Plotting training accuracy\n",
        "    sns.lineplot(x=range(1, final_epoch + 2), y= CNN_history.history['accuracy'], label='Training accuracy')\n",
        "\n",
        "    # Plotting validation accuracy\n",
        "    sns.lineplot(x=range(1, final_epoch + 2), y= CNN_history.history['val_accuracy'], label='Validation accuracy')\n",
        "\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Training and Validation accuracy of 1DTempCNNs Over Epochs')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    CNN_model.summary()\n",
        "\n",
        "    #Testing Accuracy\n",
        "    landcover_testing_predicted = CNN_model.predict(Features_testing_CNN)\n",
        "    # Finding the index of the highest probability for each sample\n",
        "    max_prob_indices = np.argmax(landcover_testing_predicted, axis=1)\n",
        "\n",
        "    # Creating a binary array with the same shape as landcover_testing_predicted\n",
        "    binary_array = np.zeros_like(landcover_testing_predicted)\n",
        "\n",
        "    # Setting the value of 1 at the index of the highest probability for each sample\n",
        "    binary_array[np.arange(len(landcover_testing_predicted)), max_prob_indices] = 1\n",
        "\n",
        "    if Prediction_only == False:\n",
        "      print(\"Recall score for test dataset: \", sklearn.metrics.recall_score(landcover_testing_CNN, binary_array, average = 'macro'))\n",
        "      print(\"Precision score for test dataset: \", sklearn.metrics.precision_score(landcover_testing_CNN, binary_array, average = 'macro'))\n",
        "      print(\"F1 score for test dataset: \", sklearn.metrics.f1_score(landcover_testing_CNN, binary_array, average = 'macro'))\n",
        "      print(\"ROC-AUC score for test dataset: \", sklearn.metrics.roc_auc_score(landcover_testing_CNN, binary_array, average = 'macro', multi_class = 'ovr'))\n",
        "\n",
        "      print(sklearn.metrics.classification_report(landcover_testing_CNN, binary_array))\n",
        "\n",
        "      # Creating an empty dictionary to store ROC-AUC scores for each class\n",
        "      roc_auc_scores = {}\n",
        "\n",
        "      # Iterating over each class\n",
        "      for class_index in range(num_classes):\n",
        "          # Extracting the binary predictions for the current class\n",
        "          binary_predictions_class = binary_array[:, class_index]\n",
        "          true_labels_class = landcover_testing_CNN[:, class_index]\n",
        "\n",
        "          # Computing ROC-AUC score for the current class\n",
        "          roc_auc = sklearn.metrics.roc_auc_score(true_labels_class, binary_predictions_class)\n",
        "\n",
        "          # Storing the ROC-AUC score for the current class in the dictionary\n",
        "          roc_auc_scores[f'Class {class_index}'] = roc_auc\n",
        "\n",
        "      # Printing ROC-AUC scores for each class\n",
        "      for class_name, roc_auc in roc_auc_scores.items():\n",
        "          print(f'ROC AUC for {class_name}: {roc_auc}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying the 1DTempCNNs model"
      ],
      "metadata": {
        "id": "mqVyKGBeaI-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Following the goal set the inputs of the 1DTempCNNs model:\n",
        "# If only four bands are being used, use the inputs train_samples, val_samples, test_samples; other wise use the inputs Standarizad_train_samples, Standarizad_val_samples, Standarizad_test_samples\n",
        "# Also, do not forget to correspondingly change the time_steps and num_features parameters\n",
        "TempCNNs(Features_training_CNN = train_samples, Features_val_CNN = val_samples, Features_testing_CNN = test_samples, landcover_training_CNN = train_labels_encoded, landcover_val_CNN = val_labels_encoded, landcover_testing_CNN = test_labels_encoded, time_steps = train_samples.shape[2], num_features = train_samples.shape[1], num_classes = 13, Prediction_only = False)\n"
      ],
      "metadata": {
        "id": "l-qgb60FZtOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the 1DSpecCNNs model with the optimal architecture"
      ],
      "metadata": {
        "id": "floQVcYGaSdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1DSpecCNNs model\n",
        "def SpecCNNs(Features_training_CNN, Features_val_CNN, Features_testing_CNN, landcover_training_CNN, landcover_val_CNN, landcover_testing_CNN, time_steps, num_features, num_classes, Prediction_only = False):\n",
        "\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "    # Designing the architecture for the 1DSpecCNNs model\n",
        "    CNN_model = Sequential()\n",
        "\n",
        "    CNN_model.add(Conv1D(64, kernel_size=3, activation='relu', padding='same', input_shape=(time_steps, num_features)))\n",
        "    CNN_model.add(Dropout(0.1))\n",
        "    CNN_model.add(InceptionLayer((32, 32, 128, 128, 256, 256, 64, 64, 32, 32, 256, 128, 128)))\n",
        "    CNN_model.add(Dropout(0.1))\n",
        "    # Flatten layer\n",
        "    CNN_model.add(Flatten())\n",
        "\n",
        "    # Fully connected layers\n",
        "    CNN_model.add(Dense(512, activation='relu'))\n",
        "    # Output layer\n",
        "    CNN_model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    # Compiling the 1DSpecCNNs model\n",
        "    CNN_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    CNN_model.summary()\n",
        "\n",
        "    # Training the 1DSpecCNNs model\n",
        "    CNN_history = CNN_model.fit(Features_training_CNN, landcover_training_CNN, epochs=100, batch_size=32, validation_data=(Features_val_CNN, landcover_val_CNN),  callbacks=[early_stopping])\n",
        "\n",
        "    # Accessing the stopped epoch\n",
        "    final_epoch = early_stopping.stopped_epoch\n",
        "    print(\"Training stopped at epoch:\", final_epoch + 1)\n",
        "\n",
        "    sns.set(font_scale=1)\n",
        "    sns.set_style(\"darkgrid\")\n",
        "    plt.figure(figsize=(15, 4))\n",
        "\n",
        "    print('The average accuracy over all epochs: ', sum(CNN_history.history['val_accuracy']) / len(CNN_history.history['val_accuracy']))\n",
        "\n",
        "    # Plotting training accuracy\n",
        "    sns.lineplot(x=range(1, final_epoch + 2), y= CNN_history.history['accuracy'], label='Training accuracy')\n",
        "\n",
        "    # Plotting validation accuracy\n",
        "    sns.lineplot(x=range(1, final_epoch + 2), y= CNN_history.history['val_accuracy'], label='Validation accuracy')\n",
        "\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Training and Validation accuracy of 1DTempCNNs Over Epochs')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    CNN_model.summary()\n",
        "\n",
        "    #Testing Accuracy\n",
        "    landcover_testing_predicted = CNN_model.predict(Features_testing_CNN)\n",
        "    # Finding the index of the highest probability for each sample\n",
        "    max_prob_indices = np.argmax(landcover_testing_predicted, axis=1)\n",
        "\n",
        "    # Creating a binary array with the same shape as landcover_testing_predicted\n",
        "    binary_array = np.zeros_like(landcover_testing_predicted)\n",
        "\n",
        "    # Setting the value of 1 at the index of the highest probability for each sample\n",
        "    binary_array[np.arange(len(landcover_testing_predicted)), max_prob_indices] = 1\n",
        "\n",
        "    if Prediction_only == False:\n",
        "      print(\"Recall score for test dataset: \", sklearn.metrics.recall_score(landcover_testing_CNN, binary_array, average = 'macro'))\n",
        "      print(\"Precision score for test dataset: \", sklearn.metrics.precision_score(landcover_testing_CNN, binary_array, average = 'macro'))\n",
        "      print(\"F1 score for test dataset: \", sklearn.metrics.f1_score(landcover_testing_CNN, binary_array, average = 'macro'))\n",
        "      print(\"ROC-AUC score for test dataset: \", sklearn.metrics.roc_auc_score(landcover_testing_CNN, binary_array, average = 'macro', multi_class = 'ovr'))\n",
        "\n",
        "      print(sklearn.metrics.classification_report(landcover_testing_CNN, binary_array))\n",
        "\n",
        "      # Creating an empty dictionary to store ROC-AUC scores for each class\n",
        "      roc_auc_scores = {}\n",
        "\n",
        "      # Iterating over each class\n",
        "      for class_index in range(num_classes):\n",
        "          # Extracting the binary predictions for the current class\n",
        "          binary_predictions_class = binary_array[:, class_index]\n",
        "          true_labels_class = landcover_testing_CNN[:, class_index]\n",
        "\n",
        "          # Computing ROC-AUC score for the current class\n",
        "          roc_auc = sklearn.metrics.roc_auc_score(true_labels_class, binary_predictions_class)\n",
        "\n",
        "          # Storing the ROC-AUC score for the current class in the dictionary\n",
        "          roc_auc_scores[f'Class {class_index}'] = roc_auc\n",
        "\n",
        "      # Printing ROC-AUC scores for each class\n",
        "      for class_name, roc_auc in roc_auc_scores.items():\n",
        "          print(f'ROC AUC for {class_name}: {roc_auc}')\n"
      ],
      "metadata": {
        "id": "7POdPKEr8FPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying the 1DSpecCNNs model"
      ],
      "metadata": {
        "id": "ri3Evgt-anyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Following the goal set the inputs of the 1DSpecCNNs model:\n",
        "# If only four bands are being used, use the inputs train_samples, val_samples, test_samples; other wise use the inputs Standarizad_train_samples, Standarizad_val_samples, Standarizad_test_samples\n",
        "# Also, do not forget to correspondingly change the time_steps and num_features parameters\n",
        "SpecCNNs(Features_training_CNN = train_samples, Features_val_CNN = val_samples, Features_testing_CNN = test_samples, landcover_training_CNN = train_labels_encoded, landcover_val_CNN = val_labels_encoded, landcover_testing_CNN = test_labels_encoded, time_steps = train_samples.shape[1], num_features = train_samples.shape[2], num_classes = 13, Prediction_only = False)\n",
        "\n"
      ],
      "metadata": {
        "id": "aGybsi_ZadZZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "authorship_tag": "ABX9TyO2N9RO2m/m8A+ukMpntAou",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}