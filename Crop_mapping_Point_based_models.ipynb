{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nmg1994/Crop_mapping/blob/main/Crop_mapping_Point_based_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Intalling necessary packages"
      ],
      "metadata": {
        "id": "4HHAsBHrsIus"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGq9m0exOMJ0"
      },
      "outputs": [],
      "source": [
        "!pip install tqdm\n",
        "!pip install earthengine-api"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing necessary libraries/modules"
      ],
      "metadata": {
        "id": "mOtxftSxsEEz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JfpJjlPhTAPO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pylab as plt\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import ast\n",
        "import math\n",
        "import random\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import sklearn.metrics\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import jaccard_score\n",
        "from sklearn.model_selection import GridSearchCV, KFold\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import datetime\n",
        "from datetime import timedelta\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mounting Google Drive"
      ],
      "metadata": {
        "id": "nDddAJUfr_HL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i52NshMsTNdY"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connecting to Google Earth Engine (GEE) through Earth Engine Python API"
      ],
      "metadata": {
        "id": "9dDFmjm7r7Dj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2PVBehE4jNe"
      },
      "outputs": [],
      "source": [
        "import ee\n",
        "ee.Authenticate()\n",
        "ee.Initialize(project='ee-navidmehdizade73nm')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting the period for downloading the Sentinel-2 satellite images"
      ],
      "metadata": {
        "id": "9N8GCiPIrxTM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbxVU--jMVKk"
      },
      "outputs": [],
      "source": [
        "def prompt_for_date(prompt_message):\n",
        "    while True:\n",
        "        date_str = input(prompt_message)\n",
        "        try:\n",
        "            # Try to create a datetime object from the input\n",
        "            datetime.datetime.strptime(date_str, '%Y-%m-%d')\n",
        "            return date_str\n",
        "        except ValueError:\n",
        "            # If there is a ValueError, it means the format is incorrect\n",
        "            print(\"Incorrect date format, should be YYYY-MM-DD. Please try again.\")\n",
        "\n",
        "# Prompting the user for start and end dates\n",
        "start_date_str = prompt_for_date(\"Enter the start date (YYYY-MM-DD): \") # e.g., 2021-04-01\n",
        "end_date_str = prompt_for_date(\"Enter the end date (YYYY-MM-DD): \") # e.g., 2021-10-31\n",
        "\n",
        "start_date = ee.Date(start_date_str)\n",
        "end_date = ee.Date(end_date_str)\n",
        "\n",
        "# Calculating number of 5-day intervals in the given period for downloading the Sentinel-2\n",
        "number_of_intervals = end_date.difference(start_date, 'day').divide(5).ceil().getInfo()\n",
        "print('number_of_intervals: ', number_of_intervals)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting bands' values of the Sentinel-2 satellite imagery for the point samples"
      ],
      "metadata": {
        "id": "3ChrAi-droa7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVbc-RIVe-Zy"
      },
      "outputs": [],
      "source": [
        "# Initializing point shapefile after uploading the samples into assets of the GEE\n",
        "point_shapefile = ee.FeatureCollection('projects/ee-navidmehdizade73nm/assets/LandC_Crops_samples_quebec')\n",
        "\n",
        "# Function to extract maximum values of Sentinel-2 bands for a given date range\n",
        "def extract_max_values(feature, start_date, end_date):\n",
        "    filtered_collection = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \\\n",
        "        .filterDate(start_date, end_date) \\\n",
        "        .filterBounds(feature.geometry()) \\\n",
        "        .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20))\n",
        "\n",
        "    renamed_collection = filtered_collection.select(['B2', 'B3', 'B4', 'B8'], ['Blue', 'Green', 'Red', 'NIR']) # extracting values of four bands, namely Blue, Green, Red, and NIR\n",
        "\n",
        "    max_values = renamed_collection \\\n",
        "        .reduce(ee.Reducer.max()) \\\n",
        "        .reduceRegion(\n",
        "            reducer=ee.Reducer.max(),\n",
        "            geometry=feature.geometry(),\n",
        "            scale=10\n",
        "        )\n",
        "\n",
        "    return feature.set(max_values)\n",
        "\n",
        "# Function to process a specific interval\n",
        "def process_interval(index, start_date, end_date, point_shapefile):\n",
        "    interval = 5  # days\n",
        "    offset = start_date.advance(interval * index, 'day')\n",
        "    interval_start = offset\n",
        "    interval_end = offset.advance(interval, 'day')\n",
        "\n",
        "    if interval_end.difference(end_date, 'day').gt(0):\n",
        "        interval_end = end_date\n",
        "\n",
        "    processed_interval = point_shapefile.map(\n",
        "        lambda feature: extract_max_values(feature, interval_start, interval_end)\n",
        "    )\n",
        "\n",
        "    task = ee.batch.Export.table.toDrive(\n",
        "        collection=processed_interval,\n",
        "        description=f'Processed_Interval_{index}',\n",
        "        folder='Point_samples',\n",
        "        fileFormat='CSV'\n",
        "    )\n",
        "    task.start()\n",
        "\n",
        "    print(f'Processed interval {index}')\n",
        "\n",
        "for ind in range(number_of_intervals):\n",
        "    process_interval(ind, start_date, end_date, point_shapefile)\n",
        "\n",
        "print('Pay attention: Although it appears that the code has finished executing, the .csv files will take a few minutes to appear in your Google Drive, as they are still being prepared and processed in Google Earth Engine!')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing the extracted bands' value and saving them in one excel file"
      ],
      "metadata": {
        "id": "EbGQI6KvrcgM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-Bl_7gT_33K"
      },
      "outputs": [],
      "source": [
        "# defining two functions to extract the coordinate of the samples\n",
        "def extract_latitude(s):\n",
        "    try:\n",
        "        parts = s.split('[')[1].split(']')[0]\n",
        "        lat = parts.split(',')[1]\n",
        "        return float(lat)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing string: {s}, Error: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_longitude(s):\n",
        "    try:\n",
        "        parts = s.split('[')[1].split(']')[0]\n",
        "        lon = parts.split(',')[0]\n",
        "        return float(lon)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing string: {s}, Error: {e}\")\n",
        "        return None\n",
        "\n",
        "# reading the .csv files, extracting the coordinate of the samples, and preprocessing them to save all files into one excel file\n",
        "for t in range(number_of_intervals):\n",
        "  S2_5_days_df = pd.read_csv(f\"/content/drive/My Drive/Point_samples/Processed_Interval_{t}.csv\")\n",
        "\n",
        "  S2_5_days_df['latitude'] = S2_5_days_df['.geo'].apply(extract_latitude)\n",
        "  S2_5_days_df['longitude'] = S2_5_days_df['.geo'].apply(extract_longitude)\n",
        "  S2_5_days_df.drop(columns=['.geo', 'system:index'], inplace=True)\n",
        "\n",
        "  replacements = {\n",
        "      'Blue_max': f'S2_Img{t}_Blue',\n",
        "      'Green_max': f'S2_Img{t}_Green',\n",
        "      'NIR_max': f'S2_Img{t}_NIR',\n",
        "      'Red_max': f'S2_Img{t}_Red'\n",
        "  }\n",
        "\n",
        "  S2_5_days_df.rename(columns=replacements, inplace=True)\n",
        "\n",
        "  if t == 0:\n",
        "    Preprocessed_Point_samples = S2_5_days_df\n",
        "  else:\n",
        "    S2_5_days_df = S2_5_days_df.drop(columns= ['CID','latitude','longitude'])\n",
        "    Preprocessed_Point_samples = pd.concat([Preprocessed_Point_samples, S2_5_days_df], axis=1)\n",
        "\n",
        "\n",
        "Preprocessed_Point_samples.to_csv('/content/Preprocessed_Point_samples.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnXv91UQnxh9"
      },
      "outputs": [],
      "source": [
        "# Conducting undersampling to make the balance among the classes\n",
        "DF_Sentinel = pd.read_csv('/content/Preprocessed_Point_samples.csv')\n",
        "DF_Sentinel_CID_sorted = DF_Sentinel.sort_values(by = 'CID', axis = 0, ascending=True).dropna().reset_index(drop = True)\n",
        "print('DF_Sentinel_CID_sorted shape: ', DF_Sentinel_CID_sorted.shape)\n",
        "Num_img_S2 = sum(('Blue') in column for column in DF_Sentinel_CID_sorted.columns)\n",
        "\n",
        "num_uni_CID = DF_Sentinel_CID_sorted['CID'].unique()\n",
        "\n",
        "for u in num_uni_CID:\n",
        " print(f'Count of sample with CID {u}: ', (DF_Sentinel_CID_sorted['CID'] == u).sum())\n",
        " if u == num_uni_CID[0]:\n",
        "  Min_num_sample = (DF_Sentinel_CID_sorted['CID'] == u).sum()\n",
        " else:\n",
        "  if (DF_Sentinel_CID_sorted['CID'] == u).sum() < Min_num_sample:\n",
        "    Min_num_sample = (DF_Sentinel_CID_sorted['CID'] == u).sum()\n",
        "\n",
        "print('Min_num_sample: ', Min_num_sample)\n",
        "\n",
        "for w in num_uni_CID:\n",
        "  sampled_DF_Sentinel_CID_sorted = DF_Sentinel_CID_sorted[DF_Sentinel_CID_sorted['CID'] == w].sample(Min_num_sample, replace=False)\n",
        "\n",
        "  if w == num_uni_CID[0]:\n",
        "    DF_Sentinel_CID_equal_samples = sampled_DF_Sentinel_CID_sorted\n",
        "  else:\n",
        "    DF_Sentinel_CID_equal_samples = pd.concat([DF_Sentinel_CID_equal_samples, sampled_DF_Sentinel_CID_sorted], axis=0, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pR7djoPcE_3R"
      },
      "outputs": [],
      "source": [
        "# Defining a function to preprocess data and make them suitable for feeding in the ML models\n",
        "def preprocessing_data(samples_dataframe, Prediction_purpose = False):\n",
        "\n",
        "  print('samples_dataframe shape: ', samples_dataframe.shape)\n",
        "  samples_dataframe = samples_dataframe.sort_values(by = ['CID']).reset_index(drop = True)\n",
        "\n",
        "  sample_id = list (range(1,1 + samples_dataframe.shape[0]))\n",
        "  samples_dataframe['Sample_ID']=sample_id\n",
        "\n",
        "  Counts_img_S2 = sum(('Blue') in column for column in samples_dataframe.columns)\n",
        "  print('Counts_img_S2: ', Counts_img_S2)\n",
        "\n",
        "  S2_Img_ind = 0\n",
        "  Img_ind_S2 = [str(number) for number in list(range(0, int(Counts_img_S2)))]\n",
        "\n",
        "  for m_S2 in Img_ind_S2:\n",
        "    col_title_S2 = '(S2_Img'+ m_S2+'_)'\n",
        "    DF_each_img = samples_dataframe.filter(regex='(CID)|(Sample_ID)|'+ col_title_S2).reset_index(drop = True)\n",
        "    DF_each_img['Image_index'] = S2_Img_ind\n",
        "    DF_each_img.columns = DF_each_img.columns.str.replace('S2_Img'+ m_S2+'_', 'S2_')\n",
        "\n",
        "    if(m_S2 == '0'):\n",
        "      Samples_preprocessed = DF_each_img\n",
        "    else:\n",
        "      Samples_preprocessed = pd.concat([Samples_preprocessed, DF_each_img], axis=0, ignore_index=True)\n",
        "\n",
        "\n",
        "    S2_Img_ind += 1\n",
        "\n",
        "\n",
        "  raw_bands = ['S2_Blue', 'S2_Green', 'S2_NIR', 'S2_Red']\n",
        "  Samples_preprocessed[raw_bands] = Samples_preprocessed[raw_bands].astype('float64')\n",
        "\n",
        "  # Calculation of vegetation indices (VI), NDWI, and BI\n",
        "  # NDVI\n",
        "  Samples_preprocessed['NDVI'] = Samples_preprocessed.apply(lambda row: ((row.S2_NIR - row.S2_Red) / (row.S2_NIR + row.S2_Red)), axis=1)\n",
        "  # GNDVI\n",
        "  Samples_preprocessed['GNDVI'] = Samples_preprocessed.apply(lambda row: ((row.S2_NIR - row.S2_Green) / (row.S2_NIR + row.S2_Green)), axis=1)\n",
        "  # GSAVI\n",
        "  Samples_preprocessed['GSAVI'] = Samples_preprocessed.apply(lambda row: ((row.S2_NIR - row.S2_Green) / (row.S2_NIR + row.S2_Green + 0.5)) * 1.5, axis=1)\n",
        "  # GOSAVI\n",
        "  Samples_preprocessed['GOSAVI'] = Samples_preprocessed.apply(lambda row: ((row.S2_NIR - row.S2_Green) / (row.S2_NIR + row.S2_Green + 0.16)), axis=1)\n",
        "  # SAVI\n",
        "  Samples_preprocessed['SAVI'] = Samples_preprocessed.apply(lambda row: ((row.S2_NIR - row.S2_Red) / (row.S2_NIR + row.S2_Red + 0.5)) * 1.5, axis=1)\n",
        "  # OSAVI\n",
        "  Samples_preprocessed['OSAVI'] = Samples_preprocessed.apply(lambda row: ((row.S2_NIR - row.S2_Red) / (row.S2_NIR + row.S2_Red + 0.16)) * 1.16, axis=1)\n",
        "  # EVI\n",
        "  Samples_preprocessed['EVI'] = Samples_preprocessed.apply(lambda row: 2.5 * ((row.S2_NIR - row.S2_Red) / (row.S2_NIR + 6 * row.S2_Red - 7.5 * row.S2_Blue + 1)) if (row.S2_NIR + 6 * row.S2_Red - 7.5 * row.S2_Blue + 1) != 0 else 0, axis=1)\n",
        "\n",
        "  # NDWI\n",
        "  Samples_preprocessed['NDWI'] = Samples_preprocessed.apply(lambda row: ((row.S2_Green - row.S2_NIR) / (row.S2_Green + row.S2_NIR)), axis=1)\n",
        "  # BI\n",
        "  Samples_preprocessed['BI'] = Samples_preprocessed.apply(lambda row: math.sqrt(((row.S2_Blue) ** 2) + ((row.S2_Green) ** 2) + ((row.S2_NIR) ** 2) + ((row.S2_Red) ** 2)), axis=1)\n",
        "\n",
        "\n",
        "  if Prediction_purpose == False:\n",
        "    Samples_preprocessed = Samples_preprocessed.sort_values(['CID', 'Sample_ID'], ascending=[True, True], axis = 0).reset_index(drop = True)\n",
        "  else:\n",
        "    Samples_preprocessed = Samples_preprocessed.sort_values(['Sample_ID','Image_index'], ascending=[True, True], axis = 0).reset_index(drop = True)\n",
        "\n",
        "  print('Samples_preprocessed:\\n', Samples_preprocessed)\n",
        "  return Samples_preprocessed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9CG56o_JmBQ"
      },
      "outputs": [],
      "source": [
        "# Splitting the data into training and test datasets and applying one-hot encoded on the labels\n",
        "def Preprocessing_ML(samples_with_labels, label_column = 'CID', Sample_ID= 'Sample_ID'):\n",
        "\n",
        "  # Using one-hot encoding\n",
        "  encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "  one_hot_encoded = encoder.fit_transform(np.array(samples_with_labels[label_column]).reshape(-1,1))\n",
        "  one_hot_array = np.array(one_hot_encoded)\n",
        "  print('one_hot_array shape:', one_hot_array.shape)\n",
        "\n",
        "  # Splitting training and testing datasets with the portion 70:30\n",
        "  X = np.array(samples_with_labels.drop(columns= [label_column, Sample_ID]))\n",
        "  Y = np.array(one_hot_array)\n",
        "\n",
        "  Features_training, Features_testing, labels_training, labels_testing = train_test_split(X, Y, test_size=0.3, shuffle=True, random_state=0)\n",
        "\n",
        "  # Convert input features to a float array\n",
        "  Features_training = Features_training.astype(float)\n",
        "  Features_testing = Features_testing.astype(float)\n",
        "\n",
        "  # Convert one-hot encoded labels to a numerical array (dtype=float)\n",
        "  labels_training = labels_training.astype(float)\n",
        "  labels_testing = labels_testing.astype(float)\n",
        "\n",
        "  # Training datasets\n",
        "  print('Features_training shape: ', Features_training.shape)\n",
        "  print('labels_training shape: ', labels_training.shape)\n",
        "\n",
        "  # Testing datasets\n",
        "  print('Features_testing shape: ', Features_testing.shape)\n",
        "  print('labels_testing shape: ', labels_testing.shape)\n",
        "\n",
        "  return Features_training, Features_testing, labels_training, labels_testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pRtYN3jCeQz"
      },
      "outputs": [],
      "source": [
        "Samples_preprocessed_aggregated = preprocessing_data(samples_dataframe = DF_Sentinel_CID_equal_samples)\n",
        "Features_training, Features_testing, landcover_training, landcover_testing = Preprocessing_ML(samples_with_labels = Samples_preprocessed_aggregated, label_column = 'CID', Sample_ID= 'Sample_ID')\n",
        "\n",
        "landcover_training_GBM = np.argmax(landcover_training, axis=1)\n",
        "print('landcover_training_GBM shape: ', landcover_training_GBM.shape)\n",
        "\n",
        "landcover_testing_GBM = np.argmax(landcover_testing, axis=1)\n",
        "print('landcover_testing_GBM shape: ', landcover_testing_GBM.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finding optimized values for the hyper-parameters in the RF model"
      ],
      "metadata": {
        "id": "2HegCWOxrSBL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_JHBzj5o6fQ"
      },
      "outputs": [],
      "source": [
        "parameters = {'n_estimators':np.arange(100,501,100), 'max_depth':np.arange(1,30,4)}\n",
        "RF_classifier = RandomForestClassifier(criterion=\"entropy\",random_state=0, bootstrap = True)\n",
        "RF_model_gs = GridSearchCV(RF_classifier, parameters, scoring='accuracy', cv=5, return_train_score=True, n_jobs=-1, verbose=1)\n",
        "RF_model_gs.fit(Features_training, landcover_training)\n",
        "\n",
        "RF_model_gs.best_params_\n",
        "\n",
        "n_estimators_RF = RF_model_gs.best_params_['n_estimators']\n",
        "max_depth_RF = RF_model_gs.best_params_['max_depth']\n",
        "\n",
        "print(\"n_estimators_RF:\", n_estimators_RF)\n",
        "print(\"max_depth_RF:\", max_depth_RF)\n",
        "\n",
        "# Extracting the cross-validation results\n",
        "cv_results = RF_model_gs.cv_results_\n",
        "\n",
        "mean_val_scores = cv_results['mean_test_score']\n",
        "std_val_scores = cv_results['std_test_score']\n",
        "params = cv_results['params']\n",
        "\n",
        "# Creating a DataFrame to store the results\n",
        "RF_Hyperparameters_results = pd.DataFrame({'Parameters': params, 'Mean Accuracy': mean_val_scores, 'Std Dev Accuracy': std_val_scores})\n",
        "\n",
        "RF_Hyperparameters_results['Parameters'] = RF_Hyperparameters_results['Parameters'].apply(ast.literal_eval)\n",
        "\n",
        "# Extracting 'max_depth' and 'n_estimators' from 'Parameters' column\n",
        "RF_Hyperparameters_results['max_depth'] = RF_Hyperparameters_results['Parameters'].apply(lambda x: x['max_depth'])\n",
        "RF_Hyperparameters_results['n_estimators'] = RF_Hyperparameters_results['Parameters'].apply(lambda x: x['n_estimators'])\n",
        "\n",
        "# Pivot the dataframe to create a matrix suitable for heatmap\n",
        "pivot_table = RF_Hyperparameters_results.pivot('n_estimators', 'max_depth', 'Mean Accuracy')\n",
        "\n",
        "# Plotting the heatmap\n",
        "plt.figure(figsize=(10, 2.25))\n",
        "sns.heatmap(pivot_table, annot=True, cmap='coolwarm', fmt=\".5f\")\n",
        "plt.title('Mean test accuracy score heatmap on validation datasets')\n",
        "plt.ylabel('n_estimators')\n",
        "plt.xlabel('max_depth')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RF model"
      ],
      "metadata": {
        "id": "tI1VqHQ7rKFE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2h0CAht8uBP"
      },
      "outputs": [],
      "source": [
        "def RF_classification_model(Original_samples_with_only_features, Features_training, Features_testing, landcover_training, landcover_testing, number_of_intervals, Prediction_only = False, max_depth= 29, min_samples_leaf = 1, n_estimators = 200):\n",
        "\n",
        "  RF_model = RandomForestClassifier(criterion=\"entropy\", random_state=0, bootstrap=True, max_depth= max_depth, min_samples_leaf = min_samples_leaf, n_estimators = n_estimators)#**RF_model_gs.best_params_)\n",
        "  cv_scores = cross_val_score(RF_model, Features_training, landcover_training, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "  if Prediction_only == False:\n",
        "    # Output the results\n",
        "    print(f\"CV Scores: {cv_scores}\")\n",
        "    print(f\"Mean CV Score: {cv_scores.mean()}\")\n",
        "    print(f\"Standard Deviation of CV Scores: {cv_scores.std()}\")\n",
        "\n",
        "  RF_model.fit(Features_training, landcover_training)\n",
        "\n",
        "  # Get feature importances\n",
        "  importances = RF_model.feature_importances_\n",
        "  feature_names = Original_samples_with_only_features.columns\n",
        "\n",
        "  # Removing 'Image_index' feature from the importance values and corresponding feature names\n",
        "  importances_filtered = np.delete(importances, feature_names.tolist().index('Image_index'))\n",
        "  feature_names_filtered = [name for name in feature_names if name != 'Image_index']\n",
        "\n",
        "  # Scaling the importance values using MinMaxScaler\n",
        "  scaler = MinMaxScaler()\n",
        "  importances_normalized_filtered = scaler.fit_transform(importances_filtered.reshape(-1, 1)).flatten()\n",
        "\n",
        "  # Calculating the sum of importances\n",
        "  total_importance = np.sum(importances_normalized_filtered)\n",
        "\n",
        "  # Calculating the relative importance of each feature\n",
        "  relative_importance = importances_normalized_filtered / total_importance\n",
        "\n",
        "  feature_importance_dict = dict(zip(feature_names_filtered, relative_importance))\n",
        "\n",
        "  # Sorting feature importances in descending order\n",
        "  indices = relative_importance.argsort()[::-1]\n",
        "\n",
        "  # Printing and plotting the sorted relative feature importances\n",
        "  for idx in indices:\n",
        "      print(f\"Feature: {feature_names_filtered[idx]}, Relative Importance: {relative_importance[idx]}\")\n",
        "\n",
        "  # Setting font properties\n",
        "  font = {'family': 'serif', 'weight': 'normal', 'size': 12}\n",
        "\n",
        "  # Printing and plotting the sorted relative feature importances\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  sns.barplot(x=[feature_names_filtered[idx] for idx in indices], y=relative_importance[indices], palette=\"viridis\", legend=False)\n",
        "  plt.xticks(rotation=90)\n",
        "  plt.xlabel(\"Feature\", fontdict=font)\n",
        "  plt.ylabel(\"Relative Importance\", fontdict=font)\n",
        "  plt.title(\"Random Forest Relative Feature Importances (Normalized to 1)\", fontdict=font)\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "  # plotting the heatmap of features importance\n",
        "  features = list(feature_importance_dict.keys())\n",
        "  importance_values = list(feature_importance_dict.values())\n",
        "\n",
        "  # Plotting heatmap of feature importance with font settings\n",
        "  plt.figure(figsize=(8,2))\n",
        "  sns.heatmap(data=[importance_values], annot=True, fmt='.4f', cmap='YlGnBu', xticklabels=features, yticklabels=False, annot_kws={'fontproperties': font, 'rotation': 90}, cbar_kws={'aspect': 8})\n",
        "  plt.xlabel('Features', fontdict=font)\n",
        "  plt.ylabel('Importance', fontdict=font)\n",
        "  plt.title('Feature Importance Heatmap', fontdict=font)\n",
        "  plt.show()\n",
        "\n",
        "  #Testing Accuracy\n",
        "  landcover_testing_predicted = RF_model.predict(Features_testing)\n",
        "\n",
        "  if Prediction_only == False:\n",
        "    print(\"Recall score for test dataset: \", sklearn.metrics.recall_score(landcover_testing, landcover_testing_predicted, average = 'macro'))\n",
        "    print(\"Precision score for test dataset: \", sklearn.metrics.precision_score(landcover_testing, landcover_testing_predicted, average = 'macro'))\n",
        "    print(\"F1 score for test dataset: \", sklearn.metrics.f1_score(landcover_testing, landcover_testing_predicted, average = 'macro'))\n",
        "    print(\"ROC-AUC score for test dataset: \", sklearn.metrics.roc_auc_score(landcover_testing, landcover_testing_predicted, average = 'macro', multi_class = 'ovr'))\n",
        "    print(sklearn.metrics.classification_report(landcover_testing, landcover_testing_predicted))\n",
        "\n",
        "    roc_auc_scores = []\n",
        "\n",
        "    # Iterating over each class\n",
        "    for class_index in range(landcover_testing.shape[1]):\n",
        "        # Extracting the true labels and predicted probabilities for the current class\n",
        "        y_true_class = np.array([1 if label == class_index else 0 for label in np.argmax(landcover_testing, axis=1)])\n",
        "        y_pred_class = np.array([1 if label == class_index else 0 for label in np.argmax(landcover_testing_predicted, axis=1)])  # Predicted probabilities for the current class\n",
        "\n",
        "        # Calculate the ROC-AUC score for the current class\n",
        "        roc_auc_each_class = sklearn.metrics.roc_auc_score(y_true_class, y_pred_class)\n",
        "\n",
        "        # Now, roc_auc_scores list contains the ROC-AUC score for each class\n",
        "        print(f'ROC-AUC scores for class {class_index}:', roc_auc_each_class)\n",
        "\n",
        "        # Appending the ROC-AUC score to the list\n",
        "        roc_auc_scores.append(roc_auc_each_class)\n",
        "\n",
        "  landcover_testing_predicted_prob = RF_model.predict_proba(Features_testing)\n",
        "  # Stacking the probabilities for each class\n",
        "  combined_probabilities = np.stack(landcover_testing_predicted_prob, axis=-1)\n",
        "\n",
        "  # This will give an array of shape (number of samples, 2, number of classes)\n",
        "  # We need to take the probabilities corresponding to the positive class (usually the second element in the innermost arrays)\n",
        "  positive_class_probs = combined_probabilities[:, 1, :]\n",
        "  # Assigning the label of the class with the highest probability\n",
        "  predicted_labels = np.argmax(positive_class_probs, axis=1)\n",
        "  predicted_labels_df = pd.DataFrame(predicted_labels.reshape(-1,1), columns=['CID']).reset_index(drop = True)\n",
        "\n",
        "  Y_true = np.argmax(landcover_testing, axis=1)\n",
        "  True_labels_df = pd.DataFrame(Y_true.reshape(-1,1), columns=['CID']).reset_index(drop = True)\n",
        "\n",
        "  Predicted_CID_list = []\n",
        "  True_CID_list = []\n",
        "\n",
        "  for jj in range(int(len(predicted_labels_df) / number_of_intervals)):\n",
        "    starting_ind = jj*number_of_intervals\n",
        "    ending_ind = (jj+1)*number_of_intervals\n",
        "    Predicted_CID = predicted_labels_df.iloc[starting_ind:ending_ind,0].value_counts().idxmax()\n",
        "    Predicted_CID_list.append(Predicted_CID)\n",
        "\n",
        "    True_CID = True_labels_df.iloc[starting_ind:ending_ind,0].value_counts().idxmax()\n",
        "    True_CID_list.append(True_CID)\n",
        "\n",
        "  Reshaped_Cropland_predicted_samples = pd.DataFrame(Predicted_CID_list, columns=['CID']).reset_index(drop = True)\n",
        "\n",
        "  confusion_matrix = sklearn.metrics.confusion_matrix(np.array(True_CID_list).reshape(-1), np.array(Predicted_CID_list).reshape(-1))\n",
        "\n",
        "  # class labels\n",
        "  class_labels = ['Pasture', 'Wetland', 'Water', 'Shrubland', 'Forest', 'Urban', 'Barren', 'Apple', 'Small fruits', 'Canola', 'Soy', 'Corn', 'Other crops']\n",
        "\n",
        "  # Plotting confusion matrix\n",
        "  plt.figure(figsize=(12, 12))\n",
        "  sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Reds', cbar=False, xticklabels=class_labels, yticklabels=class_labels)\n",
        "  plt.xlabel('Predicted Label')\n",
        "  plt.ylabel('True Label')\n",
        "  plt.title('Confusion matrix of the RF model for the test dataset')\n",
        "  plt.xticks(ha='right')\n",
        "  plt.yticks(rotation=0, ha='right')\n",
        "  plt.tick_params(axis='x',  top=True, bottom=False, labeltop=True, labelbottom=False)\n",
        "  plt.show()\n",
        "\n",
        "  return Reshaped_Cropland_predicted_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkO3UmC8gYzI"
      },
      "outputs": [],
      "source": [
        "# Applying RF model and predicting class labels for the test dataset\n",
        "RF_classification_model(Original_samples_with_only_features = Samples_preprocessed_aggregated.drop(columns= ['CID','Sample_ID']), Features_training = Features_training, Features_testing = Features_testing, landcover_training = landcover_training, landcover_testing = landcover_testing, number_of_intervals= Num_img_S2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finding optimized values for the hyper-parameters in the GBM model"
      ],
      "metadata": {
        "id": "1ocqgNfsqeH8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gns73Z817HKT"
      },
      "outputs": [],
      "source": [
        "GBM_parameters = {'n_estimators':np.arange(100,501,100), 'learning_rate':[0.1, 0.2, 0.4, 0.5, 0.6, 0.8, 1]}\n",
        "GBM_classifier = GradientBoostingClassifier(random_state=0)\n",
        "GBM_model_gs = GridSearchCV(GBM_classifier, GBM_parameters, scoring='accuracy', cv=5, return_train_score=True, n_jobs=-1, verbose=2)\n",
        "GBM_model_gs.fit(Features_training, landcover_training_GBM)\n",
        "\n",
        "GBM_model_gs.best_params_\n",
        "\n",
        "learning_rate_GBM = GBM_model_gs.best_params_['learning_rate']\n",
        "n_estimators_GBM = GBM_model_gs.best_params_['n_estimators']\n",
        "\n",
        "print(\"Learning rate:\", learning_rate_GBM)\n",
        "print(\"Number of estimators:\", n_estimators_GBM)\n",
        "\n",
        "# Extracting the cross-validation results\n",
        "cv_results_GBM = GBM_model_gs.cv_results_\n",
        "\n",
        "mean_val_scores_GBM = cv_results_GBM['mean_test_score']\n",
        "std_val_scores_GBM = cv_results_GBM['std_test_score']\n",
        "params_GBM = cv_results_GBM['params']\n",
        "\n",
        "# Creating a DataFrame to store the results\n",
        "GBM_Hyperparameters_results = pd.DataFrame({'Parameters': params_GBM, 'Mean Accuracy': mean_val_scores_GBM, 'Std Dev Accuracy': std_val_scores_GBM})\n",
        "\n",
        "import ast\n",
        "GBM_Hyperparameters_results['Parameters'] = GBM_Hyperparameters_results['Parameters'].apply(ast.literal_eval)\n",
        "\n",
        "# Extracting 'learning_rate' and 'n_estimators' from 'Parameters' column\n",
        "GBM_Hyperparameters_results['learning_rate'] = GBM_Hyperparameters_results['Parameters'].apply(lambda x: x['learning_rate'])\n",
        "GBM_Hyperparameters_results['n_estimators'] = GBM_Hyperparameters_results['Parameters'].apply(lambda x: x['n_estimators'])\n",
        "\n",
        "# Pivotting the dataframe to create a matrix suitable for heatmap\n",
        "pivot_table = GBM_Hyperparameters_results.pivot('n_estimators', 'learning_rate', 'Mean Accuracy')\n",
        "\n",
        "# Plotting the heatmap\n",
        "plt.figure(figsize=(10, 2.25))\n",
        "sns.heatmap(pivot_table, annot=True, cmap='coolwarm', fmt=\".5f\")\n",
        "plt.title('Mean test accuracy score heatmap on validation datasets')\n",
        "plt.ylabel('n_estimators')\n",
        "plt.xlabel('learning_rate')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GBM model"
      ],
      "metadata": {
        "id": "seem59BRqpOr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiaFe_d2S2FI"
      },
      "outputs": [],
      "source": [
        "def GBM_classification_model(Original_samples_with_only_features, Features_training, Features_testing, landcover_training, landcover_testing, number_of_intervals, Prediction_only = False, learning_rate = 0.4, n_estimators= 500):\n",
        "\n",
        "  GBM_model = GradientBoostingClassifier(learning_rate = learning_rate, n_estimators=n_estimators)\n",
        "  cv_scores = cross_val_score(GBM_model, Features_training, landcover_training, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "  if Prediction_only == False:\n",
        "    # Output the results\n",
        "    print(f\"CV Scores: {cv_scores}\")\n",
        "    print(f\"Mean CV Score: {cv_scores.mean()}\")\n",
        "    print(f\"Standard Deviation of CV Scores: {cv_scores.std()}\")\n",
        "\n",
        "  GBM_model.fit(Features_training, landcover_training)\n",
        "\n",
        "  #Testing Accuracy\n",
        "  landcover_testing_predicted = GBM_model.predict(Features_testing)\n",
        "  landcover_testing_predicted_prob = GBM_model.predict_proba(Features_testing)\n",
        "  print(landcover_testing_predicted_prob.shape)\n",
        "\n",
        "  if Prediction_only == False:\n",
        "    print(\"Recall score for test dataset: \", sklearn.metrics.recall_score(landcover_testing, landcover_testing_predicted, average = 'macro'))\n",
        "    print(\"Precision score for test dataset: \", sklearn.metrics.precision_score(landcover_testing, landcover_testing_predicted, average = 'macro'))\n",
        "    print(\"F1 score for test dataset: \", sklearn.metrics.f1_score(landcover_testing, landcover_testing_predicted, average = 'macro'))\n",
        "    print(\"ROC-AUC score for test dataset: \", sklearn.metrics.roc_auc_score(landcover_testing, landcover_testing_predicted_prob, average = 'macro', multi_class = 'ovr'))\n",
        "    print(sklearn.metrics.classification_report(landcover_testing, landcover_testing_predicted))\n",
        "\n",
        "    roc_auc_scores = []\n",
        "\n",
        "    # Iterating over each class\n",
        "    for class_index in range(13):\n",
        "        # Extracting the true labels and predicted probabilities for the current class\n",
        "        y_true_class = np.array([1 if label == class_index else 0 for label in landcover_testing])\n",
        "        y_pred_class = np.array([1 if label == class_index else 0 for label in np.argmax(landcover_testing_predicted_prob, axis= 1)])  # Predicted probabilities for the current class\n",
        "\n",
        "        # Calculating the ROC-AUC score for the current class\n",
        "        roc_auc_each_class = sklearn.metrics.roc_auc_score(y_true_class, y_pred_class)\n",
        "\n",
        "        # Now, roc_auc_scores list contains the ROC-AUC score for each class\n",
        "        print(f'ROC-AUC scores for class {class_index}:', roc_auc_each_class)\n",
        "\n",
        "        # Appending the ROC-AUC score to the list\n",
        "        roc_auc_scores.append(roc_auc_each_class)\n",
        "\n",
        "\n",
        "  predicted_labels = np.argmax(landcover_testing_predicted_prob, axis=1)\n",
        "  predicted_labels_df = pd.DataFrame(predicted_labels.reshape(-1,1), columns=['CID']).reset_index(drop = True)\n",
        "\n",
        "  Predicted_CID_list = []\n",
        "\n",
        "  for jj in range(int(len(predicted_labels_df) / number_of_intervals)):\n",
        "    starting_ind = jj*number_of_intervals\n",
        "    ending_ind = (jj+1)*number_of_intervals\n",
        "    Predicted_CID = predicted_labels_df.iloc[starting_ind:ending_ind,0].value_counts().idxmax()\n",
        "    Predicted_CID_list.append(Predicted_CID)\n",
        "\n",
        "  Reshaped_Cropland_predicted_samples = pd.DataFrame(Predicted_CID_list, columns=['CID']).reset_index(drop = True)\n",
        "\n",
        "  return Reshaped_Cropland_predicted_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXUAObl9D7EG"
      },
      "outputs": [],
      "source": [
        "#Applying GBM model and predicting class labels for the test dataset\n",
        "GBM_classification_model(Original_samples_with_only_features = Samples_preprocessed_aggregated.drop(columns= ['CID','Sample_ID']), Features_training = Features_training, Features_testing = Features_testing, landcover_training = landcover_training_GBM, landcover_testing = landcover_testing_GBM, number_of_intervals= Num_img_S2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assessing the impact of per-class sample sizes on the accuracy of point-based ML models"
      ],
      "metadata": {
        "id": "M9GoVPKjqQ9M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SX4Nb1c8ABa"
      },
      "outputs": [],
      "source": [
        "train_scores = []\n",
        "test_scores = []\n",
        "\n",
        "train_scores_GBM = []\n",
        "test_scores_GBM = []\n",
        "\n",
        "sample_sizes = np.append(np.arange(30,int(DF_Sentinel_CID_equal_samples.shape[0] / len(DF_Sentinel_CID_equal_samples['CID'].unique())),30), int(DF_Sentinel_CID_equal_samples.shape[0] / len(DF_Sentinel_CID_equal_samples['CID'].unique())))\n",
        "\n",
        "for batch_size in sample_sizes:\n",
        "\n",
        "  for each_class in DF_Sentinel_CID_equal_samples['CID'].unique():\n",
        "\n",
        "    sampled_batch_each_class = DF_Sentinel_CID_equal_samples[DF_Sentinel_CID_equal_samples['CID'] == each_class].sample(batch_size, replace=False)\n",
        "\n",
        "    if each_class == DF_Sentinel_CID_equal_samples['CID'].unique()[0]:\n",
        "      DF_Sentinel_CID_equal_samples_batch_size = sampled_batch_each_class\n",
        "    else:\n",
        "      DF_Sentinel_CID_equal_samples_batch_size = pd.concat([DF_Sentinel_CID_equal_samples_batch_size, sampled_batch_each_class], axis=0, ignore_index=True)\n",
        "\n",
        "\n",
        "  samples_preprocessed_batches = preprocessing_data(samples_dataframe = DF_Sentinel_CID_equal_samples_batch_size)\n",
        "\n",
        "  Features_training, Features_testing, landcover_training, landcover_testing = Preprocessing_ML(samples_with_labels = samples_preprocessed_batches, label_column = 'CID', Sample_ID= 'Sample_ID')\n",
        "\n",
        "  landcover_training_GBM = np.argmax(landcover_training, axis=1)\n",
        "  print('landcover_training_GBM shape: ', landcover_training_GBM.shape)\n",
        "\n",
        "  landcover_testing_GBM = np.argmax(landcover_testing, axis=1)\n",
        "  print('landcover_testing_GBM shape: ', landcover_testing_GBM.shape)\n",
        "\n",
        "  #RF model\n",
        "  RF_model = RandomForestClassifier(criterion=\"entropy\", random_state=0, bootstrap=True, max_depth= 25, min_samples_leaf = 1, n_estimators = 500)#**RF_model_gs.best_params_)\n",
        "  RF_model.fit(Features_training, landcover_training)\n",
        "\n",
        "  landcover_training_predicted = RF_model.predict(Features_training)\n",
        "  landcover_testing_predicted = RF_model.predict(Features_testing)\n",
        "\n",
        "  # Calculating accuracy scores\n",
        "  train_accuracy = sklearn.metrics.f1_score(landcover_training, landcover_training_predicted, average = 'macro')\n",
        "  test_accuracy = sklearn.metrics.f1_score(landcover_testing, landcover_testing_predicted, average = 'macro')\n",
        "\n",
        "  # Appending accuracy scores to lists\n",
        "  train_scores.append(train_accuracy)\n",
        "  test_scores.append(test_accuracy)\n",
        "\n",
        "  #GBM model\n",
        "  GBM_model = GradientBoostingClassifier(learning_rate = 0.4, n_estimators= 500)\n",
        "  GBM_model.fit(Features_training, landcover_training_GBM)\n",
        "\n",
        "  landcover_training_predicted_GBM = GBM_model.predict(Features_training)\n",
        "  landcover_testing_predicted_GBM = GBM_model.predict(Features_testing)\n",
        "\n",
        "  # Calculating accuracy scores\n",
        "  train_accuracy_GBM = sklearn.metrics.f1_score(landcover_training_GBM, landcover_training_predicted_GBM, average = 'macro')\n",
        "  test_accuracy_GBM = sklearn.metrics.f1_score(landcover_testing_GBM, landcover_testing_predicted_GBM, average = 'macro')\n",
        "\n",
        "  # Appending accuracy scores to lists\n",
        "  train_scores_GBM.append(train_accuracy_GBM)\n",
        "  test_scores_GBM.append(test_accuracy_GBM)\n",
        "\n",
        "\n",
        "print('The highest F1-score accuracy was achieved using the RF model on the sample size: ', sample_sizes[np.argmax(np.array(test_scores))])\n",
        "print('Testing F1-score accuracy value: ', np.max(np.array(test_scores)))\n",
        "\n",
        "print('The highest F1-score accuracy was achieved using GBM model on the sample size: ', sample_sizes[np.argmax(np.array(test_scores_GBM))])\n",
        "print('Testing F1-score accuracy value: ', np.max(np.array(test_scores_GBM)))\n",
        "\n",
        "sns.set(font_scale=1)\n",
        "sns.set_style(\"darkgrid\")\n",
        "plt.figure(figsize=(10, 3))\n",
        "# Plotting the accuracy scores over the number of samples\n",
        "plt.plot(sample_sizes, train_scores, linestyle='--', color='blue', label='RF training accuracy')\n",
        "plt.plot(sample_sizes, test_scores, color = 'blue', label='RF testing accuracy')\n",
        "\n",
        "plt.plot(sample_sizes, train_scores_GBM, linestyle='--', color='orange', label='GBM training accuracy')\n",
        "plt.plot(sample_sizes, test_scores_GBM, color = 'orange', label='GBM testing accuracy')\n",
        "plt.xlabel('Sample size')\n",
        "plt.ylabel('F1-score value')\n",
        "plt.title('F1-score accuracy of the ML models based on the sample size')\n",
        "plt.xticks(sample_sizes)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "4HHAsBHrsIus",
        "mOtxftSxsEEz",
        "nDddAJUfr_HL",
        "9dDFmjm7r7Dj",
        "9N8GCiPIrxTM",
        "3ChrAi-droa7",
        "EbGQI6KvrcgM",
        "2HegCWOxrSBL",
        "tI1VqHQ7rKFE",
        "1ocqgNfsqeH8",
        "seem59BRqpOr",
        "M9GoVPKjqQ9M"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPCcvKwEOlACndQbgy5FxcG",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}